"""
å¯è§†åŒ–å¢å¼ºåçš„æ•°æ®é›†ï¼Œæ£€æŸ¥è¾¹ç•Œæ¡†æ˜¯å¦æ­£ç¡®ä¿ç•™

ä½¿ç”¨æ–¹æ³•ï¼š
    python visualize_augmented.py --dataset_path /home/user/MERGE/FSW-MERGE_augmented_double --num_samples 10
"""

import os
import cv2
import argparse
import random


def draw_yolo_bbox(image, bbox_line, color=(0, 255, 0), thickness=2):
    """
    åœ¨å›¾åƒä¸Šç»˜åˆ¶YOLOæ ¼å¼çš„è¾¹ç•Œæ¡†

    Args:
        image: å›¾åƒæ•°ç»„
        bbox_line: YOLOæ ¼å¼æ ‡æ³¨è¡Œ "class_id x_center y_center width height"
        color: è¾¹ç•Œæ¡†é¢œè‰² (B, G, R)
        thickness: çº¿æ¡ç²—ç»†
    """
    h, w = image.shape[:2]
    parts = bbox_line.strip().split()

    if len(parts) != 5:
        return

    class_id = int(parts[0])
    x_center = float(parts[1])
    y_center = float(parts[2])
    width_norm = float(parts[3])
    height_norm = float(parts[4])

    # è½¬æ¢ä¸ºåƒç´ åæ ‡
    x1 = int((x_center - width_norm / 2) * w)
    y1 = int((y_center - height_norm / 2) * h)
    x2 = int((x_center + width_norm / 2) * w)
    y2 = int((y_center + height_norm / 2) * h)

    # ç»˜åˆ¶çŸ©å½¢
    cv2.rectangle(image, (x1, y1), (x2, y2), color, thickness)

    # æ·»åŠ ç±»åˆ«æ ‡ç­¾
    label = f"Class {class_id}"
    label_size, _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)
    cv2.rectangle(image, (x1, y1 - label_size[1] - 5), (x1 + label_size[0], y1), color, -1)
    cv2.putText(image, label, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)


def visualize_sample(image_path, label_path, output_path=None):
    """
    å¯è§†åŒ–å•å¼ å›¾åƒåŠå…¶æ ‡æ³¨

    Args:
        image_path: å›¾åƒè·¯å¾„
        label_path: æ ‡ç­¾è·¯å¾„
        output_path: ä¿å­˜è·¯å¾„ï¼ˆå¯é€‰ï¼‰

    Returns:
        å¸¦æ ‡æ³¨çš„å›¾åƒæ•°ç»„
    """
    # è¯»å–å›¾åƒ
    image = cv2.imread(str(image_path))
    if image is None:
        print(f"âŒ æ— æ³•è¯»å–å›¾åƒ: {image_path}")
        return None

    # ç»Ÿè®¡bboxæ•°é‡
    bbox_count = 0

    # è¯»å–å¹¶ç»˜åˆ¶æ ‡æ³¨
    if os.path.exists(label_path):
        with open(label_path, 'r') as f:
            lines = f.readlines()
            bbox_count = len(lines)

            # ä¸ºä¸åŒç±»åˆ«ä½¿ç”¨ä¸åŒé¢œè‰²
            colors = [
                (0, 255, 0),    # ç»¿è‰²
                (255, 0, 0),    # è“è‰²
                (0, 0, 255),    # çº¢è‰²
                (255, 255, 0),  # é’è‰²
                (255, 0, 255),  # å“çº¢
                (0, 255, 255),  # é»„è‰²
            ]

            for line in lines:
                parts = line.strip().split()
                if len(parts) == 5:
                    class_id = int(parts[0])
                    color = colors[class_id % len(colors)]
                    draw_yolo_bbox(image, line, color=color)

    # æ·»åŠ å›¾åƒä¿¡æ¯
    h, w = image.shape[:2]
    info_text = f"Size: {w}x{h} | Bboxes: {bbox_count}"
    cv2.putText(image, info_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
    cv2.putText(image, info_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 1)

    # æ·»åŠ æ–‡ä»¶å
    filename = os.path.basename(image_path)
    cv2.putText(image, filename, (10, h - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
    cv2.putText(image, filename, (10, h - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 1)

    # ä¿å­˜å›¾åƒ
    if output_path:
        cv2.imwrite(str(output_path), image)
        print(f"âœ… ä¿å­˜åˆ°: {output_path}")

    return image


def compare_original_and_augmented(dataset_path, num_samples=10, save_dir=None):
    """
    æ¯”è¾ƒåŸå§‹å›¾åƒå’Œå¢å¼ºå›¾åƒ

    Args:
        dataset_path: æ•°æ®é›†è·¯å¾„
        num_samples: é‡‡æ ·æ•°é‡
        save_dir: ä¿å­˜ç›®å½•ï¼ˆå¯é€‰ï¼‰
    """
    img_dir = os.path.join(dataset_path, "images/Train")
    label_dir = os.path.join(dataset_path, "labels/Train")

    if not os.path.exists(img_dir):
        print(f"âŒ å›¾åƒç›®å½•ä¸å­˜åœ¨: {img_dir}")
        return

    # è·å–æ‰€æœ‰å›¾åƒæ–‡ä»¶
    all_images = [f for f in os.listdir(img_dir) if f.endswith(('.jpg', '.png', '.jpeg'))]

    # åˆ†ç¦»åŸå§‹å›¾åƒå’Œå¢å¼ºå›¾åƒ
    original_images = [f for f in all_images if '_aug_' not in f]
    augmented_images = [f for f in all_images if '_aug_' in f]

    print(f"\n{'='*70}")
    print(f"ğŸ“‚ æ•°æ®é›†è·¯å¾„: {dataset_path}")
    print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"  - åŸå§‹å›¾åƒ: {len(original_images)}")
    print(f"  - å¢å¼ºå›¾åƒ: {len(augmented_images)}")
    print(f"  - æ€»è®¡: {len(all_images)}")
    print(f"{'='*70}\n")

    # éšæœºé‡‡æ ·
    sample_originals = random.sample(original_images, min(num_samples, len(original_images)))

    if save_dir:
        os.makedirs(save_dir, exist_ok=True)
        print(f"ğŸ’¾ å¯è§†åŒ–ç»“æœå°†ä¿å­˜åˆ°: {save_dir}\n")

    # å¤„ç†æ¯ä¸ªæ ·æœ¬
    for idx, orig_img in enumerate(sample_originals, 1):
        base_name = os.path.splitext(orig_img)[0]

        print(f"[{idx}/{len(sample_originals)}] å¤„ç†: {orig_img}")

        # åŸå§‹å›¾åƒ
        orig_img_path = os.path.join(img_dir, orig_img)
        orig_label_path = os.path.join(label_dir, base_name + '.txt')

        orig_output = os.path.join(save_dir, f"compare_{idx}_original.jpg") if save_dir else None
        orig_vis = visualize_sample(orig_img_path, orig_label_path, orig_output)

        # æŸ¥æ‰¾å¯¹åº”çš„å¢å¼ºå›¾åƒ
        aug_imgs = [f for f in augmented_images if f.startswith(base_name + '_aug_')]

        if aug_imgs:
            # éšæœºé€‰æ‹©ä¸€ä¸ªå¢å¼ºç‰ˆæœ¬
            aug_img = random.choice(aug_imgs)
            aug_img_path = os.path.join(img_dir, aug_img)
            aug_label_path = os.path.join(label_dir, os.path.splitext(aug_img)[0] + '.txt')

            aug_output = os.path.join(save_dir, f"compare_{idx}_augmented.jpg") if save_dir else None
            aug_vis = visualize_sample(aug_img_path, aug_label_path, aug_output)

            # ç»Ÿè®¡bboxæ•°é‡
            orig_bbox_count = len(open(orig_label_path).readlines()) if os.path.exists(orig_label_path) else 0
            aug_bbox_count = len(open(aug_label_path).readlines()) if os.path.exists(aug_label_path) else 0

            if orig_bbox_count != aug_bbox_count:
                print(f"  âš ï¸ Bboxæ•°é‡å˜åŒ–: {orig_bbox_count} â†’ {aug_bbox_count}")
            else:
                print(f"  âœ… Bboxæ•°é‡ä¿æŒ: {orig_bbox_count}")

            # å¦‚æœä¸ä¿å­˜ï¼Œåˆ™æ˜¾ç¤º
            if not save_dir and orig_vis is not None and aug_vis is not None:
                # å¹¶æ’æ˜¾ç¤º
                combined = cv2.hconcat([orig_vis, aug_vis])
                cv2.imshow('Original (left) vs Augmented (right)', combined)
                key = cv2.waitKey(0)
                if key == ord('q'):
                    break

        print()

    if not save_dir:
        cv2.destroyAllWindows()

    print(f"âœ… å®Œæˆï¼å…±å¤„ç† {len(sample_originals)} ä¸ªæ ·æœ¬")


def analyze_dataset_statistics(dataset_path):
    """
    åˆ†ææ•°æ®é›†çš„bboxç»Ÿè®¡ä¿¡æ¯

    Args:
        dataset_path: æ•°æ®é›†è·¯å¾„
    """
    label_dir = os.path.join(dataset_path, "labels/Train")

    if not os.path.exists(label_dir):
        print(f"âŒ æ ‡ç­¾ç›®å½•ä¸å­˜åœ¨: {label_dir}")
        return

    # ç»Ÿè®¡ä¿¡æ¯
    total_images = 0
    total_bboxes = 0
    images_without_bbox = 0
    bbox_counts = []

    label_files = [f for f in os.listdir(label_dir) if f.endswith('.txt')]

    for label_file in label_files:
        label_path = os.path.join(label_dir, label_file)

        with open(label_path, 'r') as f:
            lines = [l.strip() for l in f.readlines() if l.strip()]
            bbox_count = len(lines)

        total_images += 1
        total_bboxes += bbox_count
        bbox_counts.append(bbox_count)

        if bbox_count == 0:
            images_without_bbox += 1

    # è¾“å‡ºç»Ÿè®¡
    print(f"\n{'='*70}")
    print(f"ğŸ“Š æ•°æ®é›†ç»Ÿè®¡åˆ†æ")
    print(f"{'='*70}")
    print(f"ğŸ“‚ è·¯å¾„: {label_dir}")
    print(f"\nå›¾åƒç»Ÿè®¡:")
    print(f"  - æ€»å›¾åƒæ•°: {total_images}")
    print(f"  - æ— bboxå›¾åƒ: {images_without_bbox} ({images_without_bbox/total_images*100:.2f}%)")
    print(f"\nBboxç»Ÿè®¡:")
    print(f"  - æ€»bboxæ•°: {total_bboxes}")
    print(f"  - å¹³å‡æ¯å¼ : {total_bboxes/total_images:.2f}")
    print(f"  - æœ€å°æ•°é‡: {min(bbox_counts) if bbox_counts else 0}")
    print(f"  - æœ€å¤§æ•°é‡: {max(bbox_counts) if bbox_counts else 0}")
    print(f"{'='*70}\n")

    if images_without_bbox > 0:
        print(f"âš ï¸ è­¦å‘Š: å‘ç° {images_without_bbox} å¼ å›¾åƒæ²¡æœ‰ä»»ä½•bboxï¼")
        print(f"   è¿™å¯èƒ½æ˜¯å¢å¼ºæ—¶bboxå®Œå…¨ä¸¢å¤±å¯¼è‡´çš„ã€‚")
        print(f"   å»ºè®®åˆ‡æ¢åˆ° transform_safe æ¨¡å¼ã€‚\n")


def main():
    parser = argparse.ArgumentParser(description="å¯è§†åŒ–å¢å¼ºåçš„YOLOæ•°æ®é›†")
    parser.add_argument("--dataset_path", type=str, required=True,
                        help="æ•°æ®é›†è·¯å¾„ï¼Œä¾‹å¦‚: /home/user/MERGE/FSW-MERGE_augmented_double")
    parser.add_argument("--num_samples", type=int, default=10,
                        help="éšæœºé‡‡æ ·æ•°é‡ (é»˜è®¤: 10)")
    parser.add_argument("--save_dir", type=str, default=None,
                        help="ä¿å­˜å¯è§†åŒ–ç»“æœçš„ç›®å½• (å¯é€‰)")
    parser.add_argument("--analyze_only", action="store_true",
                        help="ä»…åˆ†æç»Ÿè®¡ä¿¡æ¯ï¼Œä¸è¿›è¡Œå¯è§†åŒ–")

    args = parser.parse_args()

    # åˆ†ææ•°æ®é›†ç»Ÿè®¡
    analyze_dataset_statistics(args.dataset_path)

    # å¯è§†åŒ–æ ·æœ¬
    if not args.analyze_only:
        compare_original_and_augmented(args.dataset_path, args.num_samples, args.save_dir)


if __name__ == "__main__":
    main()

