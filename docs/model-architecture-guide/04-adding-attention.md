# 4. æ·»åŠ æ³¨æ„åŠ›å±‚æŒ‡å—

æœ¬æ–‡æ¡£è¯¦ç»†è¯´æ˜å¦‚ä½•åœ¨YOLOæ¨¡å‹ä¸­æ·»åŠ å’Œè‡ªå®šä¹‰æ³¨æ„åŠ›æœºåˆ¶ã€‚

## ğŸ¯ æ³¨æ„åŠ›æœºåˆ¶æ¦‚è§ˆ

YOLOæ¨¡å‹å·²ç»å†…ç½®äº†å¤šç§æ³¨æ„åŠ›æœºåˆ¶ï¼Œæˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•ä½¿ç”¨å®ƒä»¬ä»¥åŠå¦‚ä½•åˆ›å»ºè‡ªå®šä¹‰æ³¨æ„åŠ›å±‚ã€‚

### ç°æœ‰çš„æ³¨æ„åŠ›æœºåˆ¶

| æ³¨æ„åŠ›ç±»å‹       | æ–‡ä»¶ä½ç½®               | ä¸»è¦ç”¨é€”                     |
| ---------------- | ---------------------- | ---------------------------- |
| ChannelAttention | conv.py ç¬¬261è¡Œ        | é€šé“æ³¨æ„åŠ›ï¼ˆç±»ä¼¼SENetï¼‰      |
| SpatialAttention | conv.py ç¬¬291è¡Œ        | ç©ºé—´æ³¨æ„åŠ›                   |
| CBAM             | conv.py ç¬¬330è¡Œ        | é€šé“+ç©ºé—´æ³¨æ„åŠ›              |
| PSA              | block.py ç¬¬1854è¡Œ      | Position-Sensitive Attention |
| C2fAttn          | block.py ç¬¬305è¡Œ       | å¸¦æ³¨æ„åŠ›çš„C2fæ¨¡å—            |
| ImagePoolingAttn | block.py ç¬¬346è¡Œ       | å›¾åƒæ± åŒ–æ³¨æ„åŠ›               |
| TransformerBlock | transformer.py ç¬¬142è¡Œ | Self-attentionæœºåˆ¶           |
| AIFI             | transformer.py ç¬¬181è¡Œ | æ³¨æ„åŠ›ç‰¹å¾èåˆ               |

---

## ğŸ“¦ ä½¿ç”¨ç°æœ‰æ³¨æ„åŠ›æœºåˆ¶

### ç¤ºä¾‹1: åœ¨backboneä¸­æ·»åŠ CBAM

**CBAM (Convolutional Block Attention Module)** æ˜¯ä¸€ä¸ªè½»é‡çº§æ³¨æ„åŠ›æ¨¡å—ï¼ŒåŒ…å«é€šé“å’Œç©ºé—´æ³¨æ„åŠ›ã€‚

**æ­¥éª¤1**: æŸ¥çœ‹CBAMçš„å®šä¹‰ (`ultralytics/nn/modules/conv.py` ç¬¬330è¡Œ):

```python
class CBAM(nn.Module):
    """Convolutional Block Attention Module."""

    def __init__(self, c1, kernel_size=7):
        """Initialize CBAM with given input channel (c1) and kernel size."""
        super().__init__()
        self.channel_attention = ChannelAttention(c1)
        self.spatial_attention = SpatialAttention(kernel_size)

    def forward(self, x):
        """Applies the forward pass through C1 module."""
        return self.spatial_attention(self.channel_attention(x))
```

**æ­¥éª¤2**: åœ¨YAMLé…ç½®ä¸­ä½¿ç”¨:

**åŸå§‹YOLOv8é…ç½®**:

```yaml
backbone:
    - [-1, 1, Conv, [64, 3, 2]]
    - [-1, 1, Conv, [128, 3, 2]]
    - [-1, 3, C2f, [128, True]]
    - [-1, 1, Conv, [256, 3, 2]]
    - [-1, 6, C2f, [256, True]]
```

**æ·»åŠ CBAMå**:

```yaml
backbone:
    - [-1, 1, Conv, [64, 3, 2]]
    - [-1, 1, Conv, [128, 3, 2]]
    - [-1, 3, C2f, [128, True]]
    - [-1, 1, CBAM, [128]] # åœ¨C2fåæ·»åŠ CBAM
    - [-1, 1, Conv, [256, 3, 2]]
    - [-1, 6, C2f, [256, True]]
    - [-1, 1, CBAM, [256]] # åœ¨C2fåæ·»åŠ CBAM
```

**æ³¨æ„**: CBAMä¸æ”¹å˜é€šé“æ•°ï¼Œæ‰€ä»¥å¯ä»¥ç›´æ¥æ’å…¥ã€‚

### ç¤ºä¾‹2: ä½¿ç”¨C2fAttnæ›¿ä»£C2f

**C2fAttn** æ˜¯C2fçš„å¢å¼ºç‰ˆæœ¬ï¼Œå†…ç½®äº†æ³¨æ„åŠ›æœºåˆ¶ã€‚

**åŸå§‹**:

```yaml
- [-1, 6, C2f, [512, True]]
```

**æ›¿æ¢ä¸ºC2fAttn**:

```yaml
# C2fAttnå‚æ•°: [c2, n, ec, nh, gc, shortcut]
# ec: embedding channels (128)
# nh: number of heads (1)
# gc: global context channels (512)
- [-1, 6, C2fAttn, [512, True, 128, 1, 512]]
```

### ç¤ºä¾‹3: æ·»åŠ PSAï¼ˆPosition-Sensitive Attentionï¼‰

**PSA** é€‚ç”¨äºéœ€è¦ä½ç½®æ•æ„Ÿçš„æ³¨æ„åŠ›åœºæ™¯ã€‚

```yaml
backbone:
    - [-1, 1, Conv, [256, 3, 2]]
    - [-1, 6, C2f, [256, True]]
    - [-1, 1, PSA, [256]] # æ·»åŠ PSA
```

**PSAå‚æ•°è¯´æ˜**:

```python
PSA(c1, c2=None, e=0.5)
# c1: è¾“å…¥é€šé“æ•°
# c2: è¾“å‡ºé€šé“æ•°ï¼ˆé»˜è®¤ç­‰äºc1ï¼‰
# e: expansion ratio
```

---

## ğŸ†• åˆ›å»ºè‡ªå®šä¹‰æ³¨æ„åŠ›æœºåˆ¶

### è‡ªå®šä¹‰æ³¨æ„åŠ›1: Squeeze-and-Excitation (SE) æ³¨æ„åŠ›

**æ­¥éª¤1**: åœ¨ `ultralytics/nn/modules/block.py` ä¸­æ·»åŠ SEæ¨¡å—:

```python
class SEAttention(nn.Module):
    """Squeeze-and-Excitation attention module. Paper: https://arxiv.org/abs/1709.01507.
    """

    def __init__(self, channels, reduction=16):
        """Initialize SE attention.

        Args:
            channels (int): Number of input channels
            reduction (int): Reduction ratio for bottleneck
        """
        super().__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Sequential(
            nn.Linear(channels, channels // reduction, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(channels // reduction, channels, bias=False),
            nn.Sigmoid(),
        )

    def forward(self, x):
        """Apply SE attention to input tensor."""
        b, c, _, _ = x.size()
        # Squeeze: Global average pooling
        y = self.avg_pool(x).view(b, c)
        # Excitation: FC layers
        y = self.fc(y).view(b, c, 1, 1)
        # Scale: Element-wise multiplication
        return x * y.expand_as(x)
```

**æ­¥éª¤2**: åœ¨ `ultralytics/nn/modules/block.py` çš„ `__all__` ä¸­æ·»åŠ :

```python
__all__ = (
    # ... å…¶ä»–æ¨¡å—
    "SEAttention",  # æ–°å¢
)
```

**æ­¥éª¤3**: åœ¨ `ultralytics/nn/modules/__init__.py` ä¸­å¯¼å…¥:

```python
from .block import (
    # ... å…¶ä»–å¯¼å…¥
    SEAttention,  # æ–°å¢
)

__all__ = (
    # ... å…¶ä»–
    "SEAttention",  # æ–°å¢
)
```

**æ­¥éª¤4**: åœ¨YAMLä¸­ä½¿ç”¨:

```yaml
backbone:
    - [-1, 1, Conv, [256, 3, 2]]
    - [-1, 6, C2f, [256, True]]
    - [-1, 1, SEAttention, [256, 16]] # [channels, reduction]
```

### è‡ªå®šä¹‰æ³¨æ„åŠ›2: Efficient Channel Attention (ECA)

**ECA** æ˜¯SEçš„æ”¹è¿›ç‰ˆæœ¬ï¼Œä½¿ç”¨1Då·ç§¯æ›¿ä»£å…¨è¿æ¥å±‚ã€‚

**æ­¥éª¤1**: åœ¨ `ultralytics/nn/modules/block.py` ä¸­æ·»åŠ :

```python
class ECAAttention(nn.Module):
    """Efficient Channel Attention. Paper: https://arxiv.org/abs/1910.03151.
    """

    def __init__(self, channels, gamma=2, b=1):
        """Initialize ECA attention.

        Args:
            channels (int): Number of input channels
            gamma (int): Parameter for adaptive kernel size
            b (int): Parameter for adaptive kernel size
        """
        super().__init__()
        # è‡ªé€‚åº”è®¡ç®—å·ç§¯æ ¸å¤§å°
        t = int(abs((math.log(channels, 2) + b) / gamma))
        k_size = t if t % 2 else t + 1

        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.conv = nn.Conv1d(1, 1, kernel_size=k_size, padding=(k_size - 1) // 2, bias=False)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        """Apply ECA attention to input tensor."""
        # Feature descriptor on the global spatial information
        y = self.avg_pool(x)

        # Two different branches of ECA module
        y = self.conv(y.squeeze(-1).transpose(-1, -2)).transpose(-1, -2).unsqueeze(-1)

        # Multi-scale information fusion
        y = self.sigmoid(y)

        return x * y.expand_as(x)
```

**éœ€è¦åœ¨æ–‡ä»¶å¼€å¤´å¯¼å…¥**:

```python

```

### è‡ªå®šä¹‰æ³¨æ„åŠ›3: Coordinate Attention (CA)

**CA** åŒæ—¶è€ƒè™‘é€šé“å’Œç©ºé—´ä¿¡æ¯ï¼Œç‰¹åˆ«é€‚åˆç§»åŠ¨ç½‘ç»œã€‚

**æ­¥éª¤1**: åœ¨ `ultralytics/nn/modules/block.py` ä¸­æ·»åŠ :

```python
class CoordAttention(nn.Module):
    """Coordinate Attention for efficient mobile network design. Paper: https://arxiv.org/abs/2103.02907.
    """

    def __init__(self, inp, oup, reduction=32):
        """Initialize Coordinate Attention.

        Args:
            inp (int): Input channels
            oup (int): Output channels
            reduction (int): Reduction ratio
        """
        super().__init__()
        self.pool_h = nn.AdaptiveAvgPool2d((None, 1))
        self.pool_w = nn.AdaptiveAvgPool2d((1, None))

        mip = max(8, inp // reduction)

        self.conv1 = nn.Conv2d(inp, mip, kernel_size=1, stride=1, padding=0)
        self.bn1 = nn.BatchNorm2d(mip)
        self.act = nn.SiLU()

        self.conv_h = nn.Conv2d(mip, oup, kernel_size=1, stride=1, padding=0)
        self.conv_w = nn.Conv2d(mip, oup, kernel_size=1, stride=1, padding=0)

    def forward(self, x):
        """Apply Coordinate Attention."""
        identity = x

        _n, _c, h, w = x.size()
        # Xæ–¹å‘æ± åŒ–
        x_h = self.pool_h(x)
        # Yæ–¹å‘æ± åŒ–
        x_w = self.pool_w(x).permute(0, 1, 3, 2)

        # æ‹¼æ¥å¹¶ç¼–ç 
        y = torch.cat([x_h, x_w], dim=2)
        y = self.conv1(y)
        y = self.bn1(y)
        y = self.act(y)

        # åˆ†å‰²å¹¶è§£ç 
        x_h, x_w = torch.split(y, [h, w], dim=2)
        x_w = x_w.permute(0, 1, 3, 2)

        # ç”Ÿæˆæ³¨æ„åŠ›æƒé‡
        a_h = self.conv_h(x_h).sigmoid()
        a_w = self.conv_w(x_w).sigmoid()

        # åº”ç”¨æ³¨æ„åŠ›
        out = identity * a_w * a_h

        return out
```

---

## ğŸ”„ åœ¨ä¸åŒä½ç½®æ·»åŠ æ³¨æ„åŠ›

### ä½ç½®1: åœ¨å·ç§¯å±‚ä¹‹å

```yaml
- [-1, 1, Conv, [256, 3, 2]]
- [-1, 1, SEAttention, [256]] # å·ç§¯åæ·»åŠ æ³¨æ„åŠ›
```

### ä½ç½®2: åœ¨C2fæ¨¡å—ä¹‹å

```yaml
- [-1, 6, C2f, [512, True]]
- [-1, 1, CBAM, [512]] # C2fåæ·»åŠ æ³¨æ„åŠ›
```

### ä½ç½®3: åœ¨SPPFä¹‹åï¼ˆbackboneæœ«å°¾ï¼‰

```yaml
- [-1, 1, SPPF, [1024, 5]]
- [-1, 1, CoordAttention, [1024, 1024]] # SPPFåæ·»åŠ æ³¨æ„åŠ›
```

### ä½ç½®4: åœ¨headä¸­çš„ç‰¹å¾èåˆå¤„

```yaml
head:
    - [-1, 1, nn.Upsample, [None, 2, "nearest"]]
    - [[-1, 6], 1, Concat, [1]]
    - [-1, 3, C2f, [512]]
    - [-1, 1, ECAAttention, [512]] # èåˆåæ·»åŠ æ³¨æ„åŠ›
```

---

## ğŸ—ï¸ åˆ›å»ºå¸¦æ³¨æ„åŠ›çš„å¤åˆæ¨¡å—

### ç¤ºä¾‹: C2fSE - å¸¦SEæ³¨æ„åŠ›çš„C2f

**æ­¥éª¤1**: åœ¨ `ultralytics/nn/modules/block.py` ä¸­å®šä¹‰:

```python
class C2fSE(C2f):
    """C2f module with Squeeze-and-Excitation attention."""

    def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5, reduction=16):
        """Initialize C2f with SE attention."""
        super().__init__(c1, c2, n, shortcut, g, e)
        # åœ¨è¾“å‡ºåæ·»åŠ SEæ³¨æ„åŠ›
        self.se = SEAttention(c2, reduction)

    def forward(self, x):
        """Forward pass with SE attention."""
        y = super().forward(x)  # è°ƒç”¨C2fçš„forward
        return self.se(y)  # åº”ç”¨SEæ³¨æ„åŠ›
```

**æ­¥éª¤2**: æ³¨å†Œå¹¶åœ¨YAMLä¸­ä½¿ç”¨:

```yaml
backbone:
    - [-1, 3, C2fSE, [256, True, 1, False, 1, 0.5, 16]]
    # å‚æ•°: [c2, shortcut, n, shortcut, g, e, reduction]
```

### ç¤ºä¾‹: ConvCBAM - å¸¦CBAMçš„å·ç§¯

```python
class ConvCBAM(nn.Module):
    """Convolution followed by CBAM attention."""

    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, d=1, act=True, kernel_size=7):
        """Initialize Conv + CBAM."""
        super().__init__()
        self.conv = Conv(c1, c2, k, s, p, g, d, act)
        self.cbam = CBAM(c2, kernel_size)

    def forward(self, x):
        """Forward pass through Conv and CBAM."""
        return self.cbam(self.conv(x))
```

---

## ğŸ“Š æ³¨æ„åŠ›æœºåˆ¶æ€§èƒ½å¯¹æ¯”

### è®¡ç®—å¤æ‚åº¦å¯¹æ¯”

| æ³¨æ„åŠ›ç±»å‹     | å‚æ•°é‡ | è®¡ç®—é‡ | æ¨ç†é€Ÿåº¦ | ç²¾åº¦æå‡ |
| -------------- | ------ | ------ | -------- | -------- |
| SE             | ä½     | ä½     | å¿«       | ä¸­ç­‰     |
| CBAM           | ä½     | ä½     | å¿«       | ä¸­ç­‰     |
| ECA            | æä½   | æä½   | æå¿«     | ä¸­ç­‰     |
| CoordAttention | ä½     | ä¸­ç­‰   | ä¸­ç­‰     | é«˜       |
| PSA            | ä¸­ç­‰   | ä¸­ç­‰   | ä¸­ç­‰     | é«˜       |
| Transformer    | é«˜     | é«˜     | æ…¢       | é«˜       |

### é€‚ç”¨åœºæ™¯å»ºè®®

1. **ç§»åŠ¨ç«¯/è¾¹ç¼˜è®¾å¤‡**:
    - ä¼˜å…ˆé€‰æ‹©: ECA, SE
    - é¿å…: Transformer, PSA

2. **æœåŠ¡å™¨ç«¯/é«˜ç²¾åº¦è¦æ±‚**:
    - æ¨è: CoordAttention, PSA, Transformer
3. **å¹³è¡¡æ€§èƒ½å’Œé€Ÿåº¦**:
    - æ¨è: CBAM, C2fAttn

---

## ğŸ¨ ç»„åˆå¤šç§æ³¨æ„åŠ›

### ä¸²è”æ³¨æ„åŠ›

```python
class MultiAttention(nn.Module):
    """Combine multiple attention mechanisms."""

    def __init__(self, channels):
        super().__init__()
        self.channel_attn = ChannelAttention(channels)
        self.spatial_attn = SpatialAttention()
        self.se_attn = SEAttention(channels)

    def forward(self, x):
        x = self.channel_attn(x)
        x = self.spatial_attn(x)
        x = self.se_attn(x)
        return x
```

### å¹¶è”æ³¨æ„åŠ›ï¼ˆåŠ æƒèåˆï¼‰

```python
class ParallelAttention(nn.Module):
    """Parallel attention with weighted fusion."""

    def __init__(self, channels):
        super().__init__()
        self.cbam = CBAM(channels)
        self.se = SEAttention(channels)
        # å¯å­¦ä¹ çš„èåˆæƒé‡
        self.alpha = nn.Parameter(torch.tensor(0.5))

    def forward(self, x):
        cbam_out = self.cbam(x)
        se_out = self.se(x)
        return self.alpha * cbam_out + (1 - self.alpha) * se_out
```

---

## âœ… éªŒè¯æ³¨æ„åŠ›æ¨¡å—

### æµ‹è¯•ä»£ç 

```python
import torch

from ultralytics.nn.modules import CoordAttention, ECAAttention, SEAttention

# åˆ›å»ºæµ‹è¯•è¾“å…¥
x = torch.randn(2, 256, 40, 40)  # [batch, channels, height, width]

# æµ‹è¯•SEæ³¨æ„åŠ›
se = SEAttention(256, reduction=16)
y_se = se(x)
print(f"SE output shape: {y_se.shape}")  # åº”è¯¥å’Œè¾“å…¥ç›¸åŒ

# æµ‹è¯•ECAæ³¨æ„åŠ›
eca = ECAAttention(256)
y_eca = eca(x)
print(f"ECA output shape: {y_eca.shape}")

# æµ‹è¯•Coordæ³¨æ„åŠ›
coord = CoordAttention(256, 256)
y_coord = coord(x)
print(f"Coord output shape: {y_coord.shape}")

# éªŒè¯è¾“å‡ºèŒƒå›´
print(f"SE output range: [{y_se.min().item():.2f}, {y_se.max().item():.2f}]")
```

---

## ğŸ“ æœ€ä½³å®è·µ

1. **æ¸è¿›å¼æ·»åŠ **: å…ˆæ·»åŠ ä¸€ä¸ªæ³¨æ„åŠ›æ¨¡å—ï¼Œè®­ç»ƒæµ‹è¯•åå†è€ƒè™‘æ·»åŠ æ›´å¤š
2. **ä½ç½®é€‰æ‹©**: é€šå¸¸åœ¨ç‰¹å¾æå–å—ä¹‹åæ·»åŠ æ•ˆæœæœ€å¥½
3. **å‚æ•°è°ƒä¼˜**: æ³¨æ„åŠ›çš„reductionå‚æ•°éœ€è¦æ ¹æ®é€šé“æ•°è°ƒæ•´
4. **é¿å…è¿‡åº¦**: è¿‡å¤šæ³¨æ„åŠ›æ¨¡å—ä¼šé™ä½é€Ÿåº¦è€Œä¸ä¸€å®šæå‡ç²¾åº¦
5. **å¯¹æ¯”å®éªŒ**: å§‹ç»ˆä¸baselineå¯¹æ¯”ï¼Œç¡®ä¿æ”¹è¿›æœ‰æ•ˆ

---

## ğŸš¨ å¸¸è§é—®é¢˜

### Q1: æ·»åŠ æ³¨æ„åŠ›åç²¾åº¦ä¸‹é™ï¼Ÿ

**A**: å¯èƒ½æ˜¯æ³¨æ„åŠ›å‚æ•°è®¾ç½®ä¸å½“ï¼Œå°è¯•è°ƒæ•´reduction ratioæˆ–kernel size

### Q2: æ¨ç†é€Ÿåº¦æ˜æ˜¾å˜æ…¢ï¼Ÿ

**A**: é¿å…ä½¿ç”¨è¿‡å¤šå¤æ‚æ³¨æ„åŠ›ï¼Œè€ƒè™‘ä½¿ç”¨ECAç­‰è½»é‡çº§æ–¹æ¡ˆ

### Q3: æ³¨æ„åŠ›æ¨¡å—ä¸èµ·ä½œç”¨ï¼Ÿ

**A**: ç¡®ä¿æ³¨æ„åŠ›çš„è¾“å‡ºè¢«æ­£ç¡®ä½¿ç”¨ï¼Œæ£€æŸ¥forwardå‡½æ•°çš„å®ç°

---

ä¸‹ä¸€æ­¥ï¼Œè¯·é˜…è¯» [æ¨¡å‹é…ç½®æ–‡ä»¶è¯¦è§£](./05-yaml-configuration.md) å­¦ä¹ å¦‚ä½•é€šè¿‡YAMLé…ç½®æ–‡ä»¶çµæ´»å®šä¹‰æ¨¡å‹ã€‚
