# 2. æ ¸å¿ƒæ¨¡å—è¯¦è§£

æœ¬æ–‡æ¡£è¯¦ç»†è§£é‡ŠYOLOæ¨¡å‹ä¸­å„ä¸ªæ ¸å¿ƒæ¨¡å—çš„å®ç°å’Œä½œç”¨ã€‚

## ğŸ“¦ å·ç§¯æ¨¡å— (conv.py)

### Conv - æ ‡å‡†å·ç§¯å—

**ä½ç½®**: `ultralytics/nn/modules/conv.py` ç¬¬38-93è¡Œ

**ç»“æ„**:

```
Conv = Conv2d + BatchNorm2d + Activation
```

**æºç è§£æ**:

```python
class Conv(nn.Module):
    default_act = nn.SiLU()  # é»˜è®¤æ¿€æ´»å‡½æ•°

    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, d=1, act=True):
        # c1: è¾“å…¥é€šé“æ•°
        # c2: è¾“å‡ºé€šé“æ•°
        # k: å·ç§¯æ ¸å¤§å°
        # s: æ­¥é•¿
        # p: paddingï¼ˆè‡ªåŠ¨è®¡ç®—å¦‚æœä¸ºNoneï¼‰
        # g: åˆ†ç»„æ•°
        # d: è†¨èƒ€ç‡
        # act: æ¿€æ´»å‡½æ•°
        super().__init__()
        self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p, d), groups=g, dilation=d, bias=False)
        self.bn = nn.BatchNorm2d(c2)
        self.act = self.default_act if act is True else act if isinstance(act, nn.Module) else nn.Identity()

    def forward(self, x):
        return self.act(self.bn(self.conv(x)))
```

**ä½¿ç”¨åœºæ™¯**:

- YOLOæ¨¡å‹ä¸­æœ€åŸºç¡€çš„å·ç§¯å•å…ƒ
- å‡ ä¹æ‰€æœ‰å±‚éƒ½åŸºäºæ­¤æ„å»º
- ä¸‹é‡‡æ ·ã€ç‰¹å¾æå–ç­‰

**ä¿®æ”¹ç¤ºä¾‹**:

```python
# ä¿®æ”¹é»˜è®¤æ¿€æ´»å‡½æ•°ä¸ºMish
Conv.default_act = nn.Mish()

# æˆ–è€…åœ¨åˆå§‹åŒ–æ—¶æŒ‡å®š
conv = Conv(64, 128, k=3, s=2, act=nn.Mish())
```

### DWConv - æ·±åº¦å¯åˆ†ç¦»å·ç§¯

**ä½ç½®**: `ultralytics/nn/modules/conv.py` ç¬¬139-152è¡Œ

**ç‰¹ç‚¹**:

- ä½¿ç”¨åˆ†ç»„å·ç§¯ï¼ˆgroups=è¾“å…¥é€šé“æ•°ï¼‰
- å‡å°‘å‚æ•°é‡å’Œè®¡ç®—é‡
- å¸¸ç”¨äºè½»é‡çº§æ¨¡å‹

**æºç **:

```python
class DWConv(Conv):
    def __init__(self, c1, c2, k=1, s=1, d=1, act=True):
        super().__init__(c1, c2, k, s, g=math.gcd(c1, c2), d=d, act=act)
```

### GhostConv - Ghostå·ç§¯

**ä½ç½®**: `ultralytics/nn/modules/conv.py` ç¬¬170-192è¡Œ

**åŸç†**:

- å…ˆç”¨å°‘é‡å·ç§¯ç”Ÿæˆç‰¹å¾
- å†ç”¨cheapæ“ä½œï¼ˆå¦‚DWå·ç§¯ï¼‰ç”Ÿæˆæ›´å¤šç‰¹å¾
- æ˜¾è‘—å‡å°‘è®¡ç®—é‡

**åº”ç”¨**: YOLOv8-ghostæ¨¡å‹

### ChannelAttention & SpatialAttention

**ä½ç½®**: `ultralytics/nn/modules/conv.py` ç¬¬261-326è¡Œ

**ChannelAttention** - é€šé“æ³¨æ„åŠ›:

```python
class ChannelAttention(nn.Module):
    def __init__(self, channels: int) -> None:
        super().__init__()
        self.pool = nn.AdaptiveAvgPool2d(1)  # å…¨å±€å¹³å‡æ± åŒ–
        self.fc = nn.Conv2d(channels, channels, 1, 1, 0, bias=True)  # 1x1å·ç§¯
        self.act = nn.Sigmoid()

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return x * self.act(self.fc(self.pool(x)))  # åŠ æƒåŸå§‹ç‰¹å¾
```

**SpatialAttention** - ç©ºé—´æ³¨æ„åŠ›:

```python
class SpatialAttention(nn.Module):
    def __init__(self, kernel_size=7):
        super().__init__()
        self.cv1 = nn.Conv2d(2, 1, kernel_size, padding=kernel_size // 2, bias=False)
        self.act = nn.Sigmoid()

    def forward(self, x):
        return x * self.act(self.cv1(torch.cat([torch.mean(x, 1, keepdim=True), torch.max(x, 1, keepdim=True)[0]], 1)))
```

**CBAM** - ç»“åˆé€šé“å’Œç©ºé—´æ³¨æ„åŠ›:

```python
class CBAM(nn.Module):
    def __init__(self, c1, kernel_size=7):
        super().__init__()
        self.channel_attention = ChannelAttention(c1)
        self.spatial_attention = SpatialAttention(kernel_size)

    def forward(self, x):
        return self.spatial_attention(self.channel_attention(x))
```

---

## ğŸ§± æ„å»ºå—æ¨¡å— (block.py)

### C2f - YOLOv8çš„æ ¸å¿ƒæ¨¡å—

**ä½ç½®**: `ultralytics/nn/modules/block.py` ç¬¬250-302è¡Œ

**ç»“æ„**:

```
C2f = Conv + n * Bottleneck + Conv + Concat
```

**è¯¦ç»†è§£æ**:

```python
class C2f(nn.Module):
    def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5):
        # c1: è¾“å…¥é€šé“
        # c2: è¾“å‡ºé€šé“
        # n: Bottlenecké‡å¤æ¬¡æ•°
        # shortcut: æ˜¯å¦ä½¿ç”¨æ®‹å·®è¿æ¥
        # g: åˆ†ç»„æ•°
        # e: æ‰©å±•æ¯”ç‡ï¼ˆä¸­é—´é€šé“ = c2 * eï¼‰
        super().__init__()
        self.c = int(c2 * e)  # éšè—é€šé“æ•°
        self.cv1 = Conv(c1, 2 * self.c, 1, 1)  # 1x1å·ç§¯æ‰©å±•é€šé“
        self.cv2 = Conv((2 + n) * self.c, c2, 1)  # 1x1å·ç§¯å‹ç¼©é€šé“
        self.m = nn.ModuleList(Bottleneck(self.c, self.c, shortcut, g, k=((3, 3), (3, 3)), e=1.0) for _ in range(n))

    def forward(self, x):
        y = list(self.cv1(x).split((self.c, self.c), 1))  # åˆ†æˆä¸¤éƒ¨åˆ†
        y.extend(m(y[-1]) for m in self.m)  # ä¾æ¬¡é€šè¿‡Bottleneck
        return self.cv2(torch.cat(y, 1))  # æ‹¼æ¥å¹¶å‹ç¼©
```

**ç‰¹ç‚¹**:

- æ¢¯åº¦åˆ†æµè®¾è®¡ï¼Œæ”¹å–„æ¢¯åº¦æµ
- æ¯”YOLOv5çš„C3æ¨¡å—æ›´å¿«
- æ›´é€‚åˆå¤§æ¨¡å‹

**åœ¨YAMLä¸­ä½¿ç”¨**:

```yaml
- [-1, 3, C2f, [256, True]] # [from, n, module, [c2, shortcut]]
```

### Bottleneck - ç“¶é¢ˆå—

**ä½ç½®**: `ultralytics/nn/modules/block.py` ç¬¬206-233è¡Œ

**ç»“æ„**:

```
Bottleneck = Conv(1x1) + Conv(3x3) + (å¯é€‰çš„æ®‹å·®è¿æ¥)
```

**æºç **:

```python
class Bottleneck(nn.Module):
    def __init__(self, c1, c2, shortcut=True, g=1, k=(3, 3), e=0.5):
        super().__init__()
        c_ = int(c2 * e)  # éšè—é€šé“
        self.cv1 = Conv(c1, c_, k[0], 1)  # é™ç»´
        self.cv2 = Conv(c_, c2, k[1], 1, g=g)  # å‡ç»´
        self.add = shortcut and c1 == c2  # æ˜¯å¦ä½¿ç”¨æ®‹å·®

    def forward(self, x):
        return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))
```

### SPPF - å¿«é€Ÿç©ºé—´é‡‘å­—å¡”æ± åŒ–

**ä½ç½®**: `ultralytics/nn/modules/block.py` ç¬¬152-178è¡Œ

**åŸç†**:

- è¿ç»­ä½¿ç”¨ç›¸åŒçš„æ± åŒ–æ ¸ï¼Œè€Œéå¹¶è¡Œå¤šä¸ªä¸åŒå°ºå¯¸
- è¾¾åˆ°ç±»ä¼¼SPPçš„æ•ˆæœä½†æ›´å¿«

**æºç **:

```python
class SPPF(nn.Module):
    def __init__(self, c1, c2, k=5):
        super().__init__()
        c_ = c1 // 2
        self.cv1 = Conv(c1, c_, 1, 1)  # é™ç»´
        self.cv2 = Conv(c_ * 4, c2, 1, 1)  # å‡ç»´
        self.m = nn.MaxPool2d(kernel_size=k, stride=1, padding=k // 2)

    def forward(self, x):
        x = self.cv1(x)
        y1 = self.m(x)
        y2 = self.m(y1)
        return self.cv2(torch.cat((x, y1, y2, self.m(y2)), 1))
```

**æ•ˆæœ**: ä¸‰æ¬¡5x5æ± åŒ– â‰ˆ ä¸€æ¬¡5x5ã€9x9ã€13x13å¹¶è¡Œæ± åŒ–

### C2fAttn - å¸¦æ³¨æ„åŠ›çš„C2f

**ä½ç½®**: `ultralytics/nn/modules/block.py` ç¬¬305-342è¡Œ

**å¢å¼º**:

- åœ¨C2fåŸºç¡€ä¸Šæ·»åŠ æ³¨æ„åŠ›æœºåˆ¶
- å¯ä»¥ä½¿ç”¨é€šé“æ³¨æ„åŠ›æˆ–å…¶ä»–æ³¨æ„åŠ›å˜ä½“

**ç»“æ„**:

```python
class C2fAttn(nn.Module):
    def __init__(self, c1, c2, n=1, ec=128, nh=1, gc=512, shortcut=False, g=1, e=0.5):
        # ec: åµŒå…¥é€šé“æ•°
        # nh: æ³¨æ„åŠ›å¤´æ•°
        # gc: å…¨å±€ä¸Šä¸‹æ–‡é€šé“æ•°
        super().__init__()
        self.c = int(c2 * e)
        self.cv1 = Conv(c1, 2 * self.c, 1, 1)
        self.cv2 = Conv((3 + n) * self.c, c2, 1)
        self.m = nn.ModuleList(Bottleneck(self.c, self.c, shortcut, g, k=((3, 3), (3, 3)), e=1.0) for _ in range(n))
        self.attn = Attention(self.c, ec, nh, gc)  # æ³¨æ„åŠ›å±‚

    def forward(self, x):
        y = list(self.cv1(x).chunk(2, 1))
        y.extend(m(y[-1]) for m in self.m)
        y.append(self.attn(y[-1]))  # æ·»åŠ æ³¨æ„åŠ›è¾“å‡º
        return self.cv2(torch.cat(y, 1))
```

---

## ğŸ” Transformerå’Œæ³¨æ„åŠ›æ¨¡å— (transformer.py)

### TransformerBlock

**ä½ç½®**: `ultralytics/nn/modules/transformer.py` ç¬¬142-178è¡Œ

**ç”¨é€”**:

- åœ¨YOLOä¸­å¼•å…¥self-attentionæœºåˆ¶
- æ•è·é•¿è·ç¦»ä¾èµ–

**æºç ç®€åŒ–**:

```python
class TransformerBlock(nn.Module):
    def __init__(self, c1, c2, num_heads, num_layers):
        super().__init__()
        self.conv = None if c1 == c2 else Conv(c1, c2)
        self.linear = nn.Linear(c2, c2)
        self.tr = nn.Sequential(*(TransformerLayer(c2, num_heads) for _ in range(num_layers)))
        self.c2 = c2

    def forward(self, x):
        if self.conv is not None:
            x = self.conv(x)
        b, _, w, h = x.shape
        p = x.flatten(2).permute(2, 0, 1)  # (w*h, b, c)
        return self.tr(p + self.linear(p)).permute(1, 2, 0).reshape(b, self.c2, w, h)
```

### AIFI - æ³¨æ„åŠ›èåˆ

**ä½ç½®**: `ultralytics/nn/modules/transformer.py` ç¬¬181-210è¡Œ

**ç‰¹ç‚¹**:

- ç”¨äºç‰¹å¾èåˆ
- å¯å­¦ä¹ çš„æ³¨æ„åŠ›æƒé‡

---

## ğŸ¯ æ£€æµ‹å¤´æ¨¡å— (head.py)

### Detect - YOLOæ£€æµ‹å¤´

**ä½ç½®**: `ultralytics/nn/modules/head.py` ç¬¬24-233è¡Œ

**æ ¸å¿ƒç»„ä»¶**:

```python
class Detect(nn.Module):
    def __init__(self, nc=80, ch=()):
        # nc: ç±»åˆ«æ•°
        # ch: è¾“å…¥é€šé“å…ƒç»„ï¼ˆæ¥è‡ªä¸åŒå°ºåº¦ï¼‰
        super().__init__()
        self.nc = nc
        self.nl = len(ch)  # æ£€æµ‹å±‚æ•°é‡
        self.reg_max = 16  # DFLé€šé“æ•°
        self.no = nc + self.reg_max * 4  # æ¯ä¸ªanchorçš„è¾“å‡ºæ•°

        # è¾¹ç•Œæ¡†å›å½’åˆ†æ”¯
        self.cv2 = nn.ModuleList(
            nn.Sequential(Conv(x, c2, 3), Conv(c2, c2, 3), nn.Conv2d(c2, 4 * self.reg_max, 1)) for x in ch
        )

        # åˆ†ç±»åˆ†æ”¯
        self.cv3 = nn.ModuleList(
            nn.Sequential(
                nn.Sequential(Conv(x, x, 3, g=x), Conv(x, c3, 1)),
                nn.Sequential(Conv(c3, c3, 3, g=c3), Conv(c3, c3, 1)),
                nn.Conv2d(c3, self.nc, 1),
            )
            for x in ch
        )

        self.dfl = DFL(self.reg_max)  # Distribution Focal Loss
```

**è¾“å‡º**:

- è¾¹ç•Œæ¡†åæ ‡ (4ä¸ªå€¼)
- ç±»åˆ«ç½®ä¿¡åº¦ (ncä¸ªå€¼)
- å¤šå°ºåº¦é¢„æµ‹ï¼ˆé€šå¸¸3ä¸ªå°ºåº¦ï¼šP3, P4, P5ï¼‰

---

## ğŸ“Š å„æ¨¡å—å¯¹æ¯”

| æ¨¡å—             | ä¸»è¦ç”¨é€”   | å‚æ•°é‡ | è®¡ç®—é‡ | ç‰¹ç‚¹       |
| ---------------- | ---------- | ------ | ------ | ---------- |
| Conv             | åŸºç¡€å·ç§¯   | ä¸­ç­‰   | ä¸­ç­‰   | æ ‡å‡†æ„å»ºå— |
| DWConv           | è½»é‡å·ç§¯   | ä½     | ä½     | ç§»åŠ¨ç«¯ä¼˜åŒ– |
| C2f              | ç‰¹å¾æå–   | é«˜     | é«˜     | YOLOv8æ ¸å¿ƒ |
| C3               | ç‰¹å¾æå–   | é«˜     | é«˜     | YOLOv5é£æ ¼ |
| SPPF             | å¤šå°ºåº¦æ± åŒ– | ä½     | ä½     | æ„Ÿå—é‡å¢å¼º |
| TransformerBlock | å…¨å±€å»ºæ¨¡   | é«˜     | æé«˜   | é•¿è·ç¦»ä¾èµ– |
| CBAM             | æ³¨æ„åŠ›     | ä½     | ä½     | ç‰¹å¾å¢å¼º   |

---

## ğŸ’¡ æ¨¡å—é€‰æ‹©å»ºè®®

### 1. éœ€è¦è½»é‡åŒ–æ¨¡å‹

```yaml
# ä½¿ç”¨DWConvæ›¿ä»£Conv
- [-1, 1, DWConv, [256, 3, 2]]
# ä½¿ç”¨GhostConv
- [-1, 1, GhostConv, [256, 3, 2]]
```

### 2. éœ€è¦æå‡æ€§èƒ½

```yaml
# æ·»åŠ æ³¨æ„åŠ›
- [-1, 1, CBAM, [256]]
# ä½¿ç”¨C2fAttn
- [-1, 3, C2fAttn, [256]]
```

### 3. éœ€è¦å¤§æ„Ÿå—é‡

```yaml
# ä½¿ç”¨SPPF
- [-1, 1, SPPF, [1024, 5]]
# æˆ–è€…æ·»åŠ Transformer
- [-1, 1, TransformerBlock, [256, 4, 2]] # [c2, num_heads, num_layers]
```

---

## ğŸ“ å°ç»“

1. **Convç³»åˆ—**: åŸºç¡€å·ç§¯æ“ä½œçš„å„ç§å˜ä½“
2. **Blockç³»åˆ—**: å¤æ‚çš„ç‰¹å¾æå–æ¨¡å—ï¼Œå¦‚C2fã€C3
3. **Attentionç³»åˆ—**: å„ç§æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¢å¼ºç‰¹å¾è¡¨è¾¾
4. **Headç³»åˆ—**: ä»»åŠ¡ç›¸å…³çš„è¾“å‡ºå¤´

ä¸‹ä¸€æ­¥ï¼Œè¯·é˜…è¯» [ä¿®æ”¹å·ç§¯å±‚æŒ‡å—](./03-modifying-conv-layers.md) å­¦ä¹ å¦‚ä½•ä¿®æ”¹å’Œè‡ªå®šä¹‰å·ç§¯å±‚ã€‚
